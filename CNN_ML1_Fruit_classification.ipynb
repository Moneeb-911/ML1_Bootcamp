{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3386d45d-0d44-4502-ac7c-c38b9807aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1feae2-2714-42f2-933f-3e5ca0a09df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b45ac38-0fbc-4a43-bb8c-64705fce6985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 862 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "data=tf.keras.preprocessing.image_dataset_from_directory(\"C:\\\\Users\\\\ALVI COMPUTER\\\\Desktop\\\\ML1_dataset\",\n",
    "                                                         shuffle=True,\n",
    "                                                         batch_size=32\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061a9955-3e86-4381-974c-83c50436a2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple', 'Banana', 'Mangoe', 'Orange']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df584c7-3abd-4959-8707-fd5c7b6b0755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b43f41-e8ce-493a-bd20-1b95a3c5679e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07988e5c-2749-4524-85f4-95aa2f3d2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "080cfec4-a088-4c85-92cd-a94b4898bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(data,shuffle=True,shuffle_size=10000):\n",
    "    if shuffle:\n",
    "        data=data.shuffle(shuffle_size,seed=12)\n",
    "        train_size=0.8*len(data)\n",
    "        train_data=data.take(int(train_size))\n",
    "        rem_data=data.skip(int(train_size))\n",
    "        test_size=len(data)*0.1\n",
    "        test_data=rem_data.take(int(test_size))\n",
    "        val_data=rem_data.skip(int(test_size))\n",
    "        return train_data,test_data,val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ede51a-bfe9-43da-8357-918e9f02df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,val_data=partition(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed47fc1c-0c9f-4250-8519-88b9abf1164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_data=test_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_data=val_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d31c276-399f-4c73-bbba-484af556f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers,models\n",
    "resize_rescale_layer=tf.keras.Sequential(layers.experimental.preprocessing.Resizing(256,256),\n",
    "                                          layers.experimental.preprocessing.Rescaling(1.0/255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b5c0645-ab7d-4fca-86b0-10e45a9e3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_layer = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34dd1d26-9af6-4bf7-bf8f-515ac291345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=3\n",
    "batch_size=32\n",
    "image_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa8850e2-6b1f-4f96-845a-d12478f9995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "input_shape=(image_size,image_size,channels)\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.experimental.preprocessing.Resizing(256,256),\n",
    "    layers.experimental.preprocessing.Rescaling(1.0/255),\n",
    "    data_augmentation_layer,\n",
    "    layers.Conv2D(32, (3, 3), strides=(1, 1), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e809194-9940-48fb-b533-1bc037ac95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adda5a85-590f-48df-825b-f986ff0ae244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "21/21 [==============================] - 63s 2s/step - loss: 1.0671 - accuracy: 0.5209 - val_loss: 0.6771 - val_accuracy: 0.7266\n",
      "Epoch 2/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.5647 - accuracy: 0.8164 - val_loss: 0.3839 - val_accuracy: 0.9219\n",
      "Epoch 3/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.4403 - accuracy: 0.8612 - val_loss: 0.3204 - val_accuracy: 0.9375\n",
      "Epoch 4/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.4851 - accuracy: 0.8134 - val_loss: 0.4900 - val_accuracy: 0.7969\n",
      "Epoch 5/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.4492 - accuracy: 0.8522 - val_loss: 0.3394 - val_accuracy: 0.9062\n",
      "Epoch 6/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.3165 - accuracy: 0.9015 - val_loss: 0.3050 - val_accuracy: 0.9297\n",
      "Epoch 7/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.3428 - accuracy: 0.8746 - val_loss: 0.2095 - val_accuracy: 0.9609\n",
      "Epoch 8/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.2608 - accuracy: 0.9313 - val_loss: 0.2381 - val_accuracy: 0.9219\n",
      "Epoch 9/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.2184 - accuracy: 0.9104 - val_loss: 0.1813 - val_accuracy: 0.9531\n",
      "Epoch 10/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.2106 - accuracy: 0.9403 - val_loss: 0.1575 - val_accuracy: 0.9609\n",
      "Epoch 11/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.1834 - accuracy: 0.9478 - val_loss: 0.2213 - val_accuracy: 0.9609\n",
      "Epoch 12/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.1838 - accuracy: 0.9448 - val_loss: 0.1720 - val_accuracy: 0.9609\n",
      "Epoch 13/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.2506 - accuracy: 0.9209 - val_loss: 0.1899 - val_accuracy: 0.9453\n",
      "Epoch 14/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.1583 - accuracy: 0.9567 - val_loss: 0.1519 - val_accuracy: 0.9609\n",
      "Epoch 15/30\n",
      "21/21 [==============================] - 55s 3s/step - loss: 0.1927 - accuracy: 0.9463 - val_loss: 0.1577 - val_accuracy: 0.9688\n",
      "Epoch 16/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.1727 - accuracy: 0.9448 - val_loss: 0.1518 - val_accuracy: 0.9531\n",
      "Epoch 17/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.1787 - accuracy: 0.9418 - val_loss: 0.1486 - val_accuracy: 0.9688\n",
      "Epoch 18/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.1303 - accuracy: 0.9597 - val_loss: 0.1094 - val_accuracy: 0.9766\n",
      "Epoch 19/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.1926 - accuracy: 0.9403 - val_loss: 0.1554 - val_accuracy: 0.9297\n",
      "Epoch 20/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.1067 - accuracy: 0.9642 - val_loss: 0.0868 - val_accuracy: 0.9766\n",
      "Epoch 21/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.1047 - accuracy: 0.9672 - val_loss: 0.0700 - val_accuracy: 0.9688\n",
      "Epoch 22/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.1326 - accuracy: 0.9582 - val_loss: 0.1509 - val_accuracy: 0.9141\n",
      "Epoch 23/30\n",
      "21/21 [==============================] - 51s 2s/step - loss: 0.1274 - accuracy: 0.9612 - val_loss: 0.0859 - val_accuracy: 0.9688\n",
      "Epoch 24/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.1116 - accuracy: 0.9701 - val_loss: 0.0679 - val_accuracy: 0.9766\n",
      "Epoch 25/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.1007 - accuracy: 0.9701 - val_loss: 0.0840 - val_accuracy: 0.9766\n",
      "Epoch 26/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.0959 - accuracy: 0.9657 - val_loss: 0.0893 - val_accuracy: 0.9766\n",
      "Epoch 27/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.1016 - accuracy: 0.9701 - val_loss: 0.1687 - val_accuracy: 0.9297\n",
      "Epoch 28/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.2973 - accuracy: 0.9164 - val_loss: 0.1959 - val_accuracy: 0.9375\n",
      "Epoch 29/30\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.1449 - accuracy: 0.9493 - val_loss: 0.1116 - val_accuracy: 0.9688\n",
      "Epoch 30/30\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.0944 - accuracy: 0.9716 - val_loss: 0.1001 - val_accuracy: 0.9609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eaacc8c400>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,\n",
    "          epochs=30,\n",
    "          batch_size=32,\n",
    "          verbose=1,\n",
    "          validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ab843d1-fe7e-4625-b40f-f99656f06abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing_2 (Resizing)       (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " rescaling_2 (Rescaling)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 127, 127, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                802880    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 896,388\n",
      "Trainable params: 896,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f2c94c5-d676-436a-a9b8-b3d2a19acea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 455ms/step - loss: 0.0264 - accuracy: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.026445861905813217, 0.984375]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe2f3e6-9f0a-4516-b509-9e03f0ebfc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48dbaa4a-ad20-497c-b85d-f02eb7fcc3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "data_fahad=tf.keras.preprocessing.image_dataset_from_directory(\"C:\\\\Users\\\\ALVI COMPUTER\\\\Downloads\\\\test_data_cnn\",\n",
    "                                                         shuffle=True,\n",
    "                                                         batch_size=10\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce8b7cd0-2f33-419e-8b7c-bfc3f68a6104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 205ms/step - loss: 2.3213 - accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.32127046585083, 0.6666666865348816]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(data_fahad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0531076-c8e3-4476-ac87-59f3a37f4ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 248ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for C:\\Users\\ALVI COMPUTER\\Downloads\\test_data_single_folder\\apple.jpeg: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     36\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mALVI COMPUTER\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtest_data_single_folder\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mpredict_images_in_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 31\u001b[0m, in \u001b[0;36mpredict_images_in_folder\u001b[1;34m(folder_path, model)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_paths[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39margmax(prediction)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(images[i])\n\u001b[1;32m---> 31\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclas[np\u001b[38;5;241m.\u001b[39margmax(predictions)]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "clas=['Apple', 'Banana', 'Mangoe', 'Orange']\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [256, 256])\n",
    "    return image\n",
    "\n",
    "def predict_images_in_folder(folder_path, model):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "\n",
    "    # Iterate through the folder and load images\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = load_and_preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "    # Convert list of images to a TensorFlow batch\n",
    "    images = tf.stack(images, axis=0)\n",
    "\n",
    "    # Predict using the model\n",
    "    predictions = model.predict(images)\n",
    "\n",
    "    # Print or return predictions\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        print(f\"Prediction for {image_paths[i]}: {np.argmax(prediction)}\")\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(f\"Prediction: {clas[np.argmax(predictions)]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'C:\\\\Users\\\\ALVI COMPUTER\\\\Downloads\\\\test_data_single_folder'\n",
    "\n",
    "predict_images_in_folder(folder_path, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traning_env",
   "language": "python",
   "name": "traning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
